"Interesting - I don't see the grant head reservation code in any ofmy performance benchmark profiling, even when running at over amillion transactions/s on a 2-socket 32-core 64-thread skylakesystem. I see other places in the transaction subsystem that arehot (e.g the CIL context lock), but not the space reservations.My initial suspect is that you have a tiny log on your testfilesystem, so it's permanently out of space and so always hittingthe slow path. Can you tell us what the storage is and it'sconfiguration? At minimum, I need to see the output of the xfs_infocommand on your test filesystem. Fixing this may simply be using alarger log on your benchmark systems.FWIW, can you post the actual profile you are seeing in the commitmessage? That helps us identify similar problems in the future, andit lets us know what paths are leading to the transactionreservation contention. i.e. this may not even be a problem with thetransaction reservation code itself.How does this impact on the strict FIFO queue behaviour the grantqueues currently have? The current code only wakes up enough waitersto consume the newly available space and it queues new waiters tothe tail of the queue. If there ever is a spurious wakeup then thewaiter that was woken from the head remains there until the nextwakeup comes in. This is intentional - spurious wakeups are rareenough we can ignore them because a) this is the slow path, and b)correctness is far more important that performance in this path.The fast path is already lockless, and we've already given uppeformance if we reach this slow path. hence we only care aboutcorrectness in this path, not performance optimisation.AFAICT the patch changes the spurious wakeup behaviour - it requeuestasks to the tail of the queue if there wasn't space available whenthey are woken, rather than leaving them as them at the head.  Theynow have to wait for all the other reservations to make progress.This breaks the guarantees of ordered forward progress the grantqueue provides permanent transaction reservations and hence opens usup to log space deadlocks because those transactions can't movetheir objects forward in the log to free up space in the log...Also, I note that wake_q_add() assumes that the wake queue is alocal stack object and so not subject to concurrency - it explicitlystates this in the code. That's not the case here - the wake queueis part of the grant head, and so is subject to extreme concurrencythat is tempered by a spin lock.  Does the wake_q code workcorrectly (e.g. have all the necessary memory barriers, etc) whenit's not a local stack object and instead protected from concurrencyby a spin lock? At minimum, the wake_q infrastructure comments anddocumentation need updating to accommodate this new use case thatwake queues are being used for.This doesn't generally doesn't happen because the space accountingtends to prevent multiple wakeups. i.e. we only wake the tasks wehave reservation space for, and log space being made available tendsto arrive in discrete chunks (because IO is slow!) such that thatpending wakeups have already been processed before the next chunk ofavailable space comes in....Yes, but they are very rare and we don't really care about this inthe slow path. If you see lots of them, it's typically a sign of aninappropriately configured filesystem for the workload being run. Ona correctly configured system, we should almost never use this slowpath....I'm betting that you'll get that and a whole lot more simply byincreasing the log size and not running the slow path at all.Where's the hunk context in your headers? You must be using anon-standard git option here.Linux kernel specific includes go in fs/xfs/xfs_linux.h, notindividual files.Why do you need to delete the ticket from the queue here? This leadsto landmines and incorrect non-FIFO behaviour....... here. This is a potential list corruption landmine because thisfunction now has unbalanced list add and removal contexts. IOWs, wecan't restart this loop without first having guaranteed the ticketis not already on the ticket queue. You need to document constraintslike this in comments and explain what code needs to guarantee thoseconstraints are met. [Because, as I noted at the end, you got thiswrong for xlog_grant_head_wake_all()]To maintian FIFO behaviour, the ticket needs to be left at the headof the grant head wait queue until it has space available to makeprogress, not get removed and requeued to the tail. Spurious wakeups are irrelevant here - forwards progress (i.e. correctness)requires FIFO ticket ordering behaviour be maintained.This push is needed to make the necessary space we are waiting onavailable in the log. Hence leaving it out of the loop you putbelow will cause the journal to get stuck in the spurious wakeuploop below and be unable to make progress. This will lead tofilesystem hangs.That's a new nested loop. Please implement it as a loop.This is buggy  - i will lead to hangs if the filesystem is shutdown and there is a spurious wakeup that triggers this to go back tosleep.The shutdown check needs to break the sleep loop.That's racy. You can't drop the spin lock betweenxlog_grant_head_wake() and xlog_grant_head_wait(), becausefree_bytes is only valid while while the spinlock is held.  Same forthe ""wake_all"" variable you added. i..e. while waking up thewaiters, we could have run out of space again and had more tasksqueued, or had the AIL tail move and now have space available.Either way, we can do the wrong thing because we dropped the lockand free_bytes and wake_all are now stale and potentially incorrect.That's another landmine. Just define the wakeq in the context whereit is used rather than use a function wide variable that requiresreinitialisation.Ok, what about xlog_grant_head_wake_all()? You didn't convert thatto use wake queues, and so that won't remove tickets for the granthead waiter list, and so those tasks will never get out of the newinner loop you added to xlog_grant_head_wait(). That meansfilesystem shutdowns will just hang the filesystem and leave itunmountable. Did you run this through fstests?Cheers,Dave--Dave Chinnerdavid@fromorbit.com",Civil
,
"Thanks for your detailed review of the patch. I now have a betterunderstanding of what should and shouldn't be done. I have sent out amore conservative v2 patchset which, hopefully, can address the concernsthat you raised.Cheers,Longman",Civil
,
"Can you please re-run and report the results for each patch on theramdisk setup? And, please, include the mkfs.xfs or xfs_info outputfor the ramdisk filesystem so I can see /exactly/ how muchconcurrency the filesystems are providing to the benchmark you arerunning.50GB is tiny for XFS. Personally, I've been using ~1PBfilesystems(*) for the performance testing I've been doingrecently...Cheers,Dave.(*) Yes, petabytes. Sparse image files on really fast SSDs are awonderful thing.--Dave Chinnerdavid@fromorbit.com",Civil
,
"I like the idea and I think it's good direction to go, but couldyou please share some from perf stat or whatever you used to meassurethe new performance?thanks,jirka",Civil
,
"Sorry but I don't like imposing a run-time check on everybody whenstack-based requests are the odd ones out.  If we're going to makethis a run-time check (I'd much prefer a compile-time check, but Iunderstand that this may involve too much churn), then please do itfor stack-based request users only.Thanks,--Email: Herbert Xu <herbert@gondor.apana.org.au>Home Page: http://gondor.apana.org.au/~herbert/PGP Key: http://gondor.apana.org.au/~herbert/pubkey.txt",Civil
,
"[ I am not subscribed to LKML, please keep me CC'd on replies ]I tried a simple test with several VMs (in my initial test, I have 48idle 1-cpu 512-mb VMs and 2 idle 2-cpu, 2-gb VMs) using libvirt, nonepinned to any CPUs. When I tried to set all of the top-level libvirt cpucgroups' to be co-scheduled (/bin/echo 1 >/sys/fs/cgroup/cpu/machine/<VM-x>.libvirt-qemu/cpu.scheduled), themachine hangs. This is using cosched_max_level=1.There are several moving parts there, so I tried narrowing it down, byonly coscheduling one VM, and thing seemed fine:/sys/fs/cgroup/cpu/machine/<VM-1>.libvirt-qemu# echo 1 > cpu.scheduled/sys/fs/cgroup/cpu/machine/<VM-1>.libvirt-qemu# cat cpu.scheduled1One thing that is not entirely obvious to me (but might be completelyintentional) is that since by default the top-level libvirt cpu cgroupsare empty:/sys/fs/cgroup/cpu/machine/<VM-1>.libvirt-qemu# cat tasksthe result of this should be a no-op, right? [This becomes relevantbelow] Specifically, all of the threads of qemu are in sub-cgroups,which do not indicate they are co-scheduling:/sys/fs/cgroup/cpu/machine/<VM-1>.libvirt-qemu# cat emulator/cpu.scheduled0/sys/fs/cgroup/cpu/machine/<VM-1>.libvirt-qemu# cat vcpu0/cpu.scheduled0When I then try to coschedule the second VM, the machine hangs./sys/fs/cgroup/cpu/machine/<VM-2>.libvirt-qemu# echo 1 > cpu.scheduledTimeout, server <HOST> not responding.On the console, I see the same backtraces I see when I try to set all ofthe VMs to be coscheduled:[  144.494091] watchdog: BUG: soft lockup - CPU#87 stuck for 22s! [CPU 0/KVM:25344][  144.507629] Modules linked in: act_police cls_basic ebtable_filter ebtables ip6table_filter iptable_filter nbd ip6table_raw ip6_tables xt_CT iptable_raw ip_tables s[  144.578858]  xxhash raid10 raid0 multipath linear raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor ses raid6_pq enclosure libcrc32c raid1 scsi[  144.599227] CPU: 87 PID: 25344 Comm: CPU 0/KVM Tainted: G           O      4.19.0-rc2-amazon-cosched+ #1[  144.608819] Hardware name: Dell Inc. PowerEdge R640/0W23H8, BIOS 1.4.9 06/29/2018[  144.616403] RIP: 0010:smp_call_function_single+0xa7/0xd0[  144.621818] Code: 01 48 89 d1 48 89 f2 4c 89 c6 e8 64 fe ff ff c9 c3 48 89 d1 48 89 f2 48 89 e6 e8 54 fe ff ff 8b 54 24 18 83 e2 01 74 0b f3 90 <8b> 54 24 18 83 e25[  144.640703] RSP: 0018:ffffb2a4a75abb40 EFLAGS: 00000202 ORIG_RAX: ffffffffffffff13[  144.648390] RAX: 0000000000000000 RBX: 0000000000000057 RCX: 0000000000000000[  144.655607] RDX: 0000000000000001 RSI: 00000000000000fb RDI: 0000000000000202[  144.662826] RBP: ffffb2a4a75abb60 R08: 0000000000000000 R09: 0000000000000f39[  144.670073] R10: 0000000000000000 R11: 0000000000000000 R12: ffff8a9c03fc8000[  144.677301] R13: ffff8ab4589dc100 R14: 0000000000000057 R15: 0000000000000000[  144.684519] FS:  00007f51cd41a700(0000) GS:ffff8ab45fac0000(0000) knlGS:0000000000000000[  144.692710] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033[  144.698542] CR2: 000000c4203c0000 CR3: 000000178a97e005 CR4: 00000000007626e0[  144.705771] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000[  144.712989] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400[  144.720215] PKRU: 55555554[  144.723016] Call Trace:[  144.725553]  ? vmx_sched_in+0xc0/0xc0 [kvm_intel][  144.730341]  vmx_vcpu_load+0x244/0x310 [kvm_intel][  144.735220]  ? __switch_to_asm+0x40/0x70[  144.739231]  ? __switch_to_asm+0x34/0x70[  144.743235]  ? __switch_to_asm+0x40/0x70[  144.747240]  ? __switch_to_asm+0x34/0x70[  144.751243]  ? __switch_to_asm+0x40/0x70[  144.755246]  ? __switch_to_asm+0x34/0x70[  144.759250]  ? __switch_to_asm+0x40/0x70[  144.763272]  ? __switch_to_asm+0x34/0x70[  144.767284]  ? __switch_to_asm+0x40/0x70[  144.771296]  ? __switch_to_asm+0x34/0x70[  144.775299]  ? __switch_to_asm+0x40/0x70[  144.779313]  ? __switch_to_asm+0x34/0x70[  144.783317]  ? __switch_to_asm+0x40/0x70[  144.787338]  kvm_arch_vcpu_load+0x40/0x270 [kvm][  144.792056]  finish_task_switch+0xe2/0x260[  144.796238]  __schedule+0x316/0x890[  144.799810]  schedule+0x32/0x80[  144.803039]  kvm_vcpu_block+0x7a/0x2e0 [kvm][  144.807399]  kvm_arch_vcpu_ioctl_run+0x1a7/0x1990 [kvm][  144.812705]  ? futex_wake+0x84/0x150[  144.816368]  kvm_vcpu_ioctl+0x3ab/0x5d0 [kvm][  144.820810]  ? wake_up_q+0x70/0x70[  144.824311]  do_vfs_ioctl+0x92/0x600[  144.827985]  ? syscall_trace_enter+0x1ac/0x290[  144.832517]  ksys_ioctl+0x60/0x90[  144.835913]  ? exit_to_usermode_loop+0xa6/0xc2[  144.840436]  __x64_sys_ioctl+0x16/0x20[  144.844267]  do_syscall_64+0x55/0x110[  144.848012]  entry_SYSCALL_64_after_hwframe+0x44/0xa9[  144.853160] RIP: 0033:0x7f51cf82bea7[  144.856816] Code: 44 00 00 48 8b 05 e1 cf 2c 00 64 c7 00 26 00 00 00 48 c7 c0 ff ff ff ff c3 66 2e 0f 1f 84 00 00 00 00 00 b8 10 00 00 00 0f 05 <48> 3d 01 f0 ff ff8[  144.875752] RSP: 002b:00007f51cd419a18 EFLAGS: 00000246 ORIG_RAX: 0000000000000010I am happy to do any further debugging I can do, or try patches on topof those posted on the mailing list.Thanks,Nish",Civil
,
"There seems to be a disconnect between what I am trying tocommunicate and what I perceive you to have understood.I'll add comments below to try to make more clear what I'm trying tosay.But first a general statement.  I understand that the intent of thepatch wording is to allow use of email addresses in the tags of a patchsubmittal or git commit without being an unacceptable behavior.  I donot think that the words in the patch accomplish that goal.The patch says ""Publishing ... electronic address not ordinarilycollected by the project, without explicit permission"".  (I think itis fair to abstract here with ""..."".)  This phrase specifies whichemail addresses can be published.  It does not specify in what casesthe email address can be published.  The desired goal is to be able topublish email addresses in patch and commit tags.Which email addresses are allowed to be published?  (This is the pointof my original comment.)  To me, the patch wording is describing howI can determine whether I can put a specific email address in a tagin a patch that I submit or commit.  I can put an email address in atag _if_ it is ""ordinarily collected by the project"".This then leads my mental process down the path of the disclosures (fromall of the companies that I do business with) that tell me what theyare going to do with my personal information, such as my address.  (Theyusually plan to share it with the world for their financial benefit.)In that context, my personal information is not _public_, but it is_ordinarily collected_ by the company.  I hope this provides someinsight into what I am reading into ""ordinarily collected by the project"".My original comment was trying to provide the concept behind a way tocreate an alternate wording in the patch to define ""which emailaddresses"".Where are email addresses allowed to be published?  I do not understandthe patch wording to address this at all.Trying to understand how you are understanding my comment vs what Iintended to communicate, it seems to me that you are focused on the""where allowed"" and I am focused on the ""which email addresses"".More clear?  Or am I still not communicating well enough?Permission vs exclusion is orthogonal to my comments.""building linux"" is not the patch wording.  ""ordinarily collected by theproject"" is a much broader universe.A very simplistic definition of public _could_ be:  - Visible on a project mail list that any one can subscribe to  - Visible on a project mail list whose archive is available via    the public internet  - Visible on an interactive communication (""chat"") platform that    is open to the public internet  - Published on a web page intended for public access (for example    this could cover opt-in conference attendee lists and emails    that conference presenters voluntarily place in their slides).  - (I am guessing the above covers 97% or more of possible public    sources, but maybe there are some more common sources.)I'm sure that the professionals that deal with information privacycould provide better wording for the above list.  I am but anamateur in that field.Anything else collected by the project would not be considered public.For example, an email address provided in an email sent to me and notcopied to any mail list would not be public.-Frank",Civil
,
"On Mon, 11 Mar 2019 11:21:10 +0100Pavel Machek <pavel@ucw.cz> wrote:I would really like to get an ack from the people who have been deep intothis first.  If you can get that, and preferably resubmit with a lesscondescending changelog, I can pick it up.Thanks,jon",Civil
,
"Hi Lee, Pi-Hsun,Missatge de Lee Jones <lee.jones@linaro.org> del dia dc., 30 de gen.2019 a les 14:07:Pi-Hsun, is this patchset still an RFC or you really want to see thismerged ASAP? If I am not mistaken there is still some work in progresstrying to push all the SCP stuff?Lee, personally I have some concerns. Looks like the cros_* family isincreasing quickly lately (cros_ec, cros_pd, cros_scp, cros_ish,cros_fp ...) and I am wondering if we are really doing well all this.To be honest, I'd like to take a deeper look before merge this, btw Ithought there was no hurry because of the RFC and I guess there arestill some scp things that are missing. I might be wrong, and ifthat's not the case I can take a look deeper and the end of the week.Best regards, Enric",Civil
,
"You are missing a cover letter from this patch set. Please have it inv2. Also use tag ""selftests/tpm2"" instead of having two tags in theshort summaries. Now they look a bit weird.8<Remove.8<Remove.8<Remove.8<Remove.8<Remove./Jarkko",Civil
,
This is phenomenal. Thank you so much for digging into this. I'm hoping this will greatly reduce the risk of future leakage.--Sent from my Android device with K-9 Mail. Please excuse my brevity.,Civil
